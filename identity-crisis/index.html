<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MediaPipe Face Detection Demo</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            background: #000;
            overflow: hidden;
            font-family: Arial, sans-serif;
        }

        #container {
            position: relative;
            width: 100vw;
            height: 100vh;
        }

        #video {
            width: 100%;
            height: 100%;
            object-fit: cover;
            transform: scaleX(-1); /* Mirror the video */
        }

        #canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
            transform: scaleX(-1); /* Mirror the canvas to match video */
        }

        #status {
            position: absolute;
            top: 20px;
            left: 20px;
            color: white;
            background: rgba(0, 0, 0, 0.7);
            padding: 10px 15px;
            border-radius: 5px;
            font-size: 14px;
            z-index: 10;
        }

        #error {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            color: #ff4444;
            background: rgba(0, 0, 0, 0.8);
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            display: none;
        }
    </style>
</head>
<body>
    <div id="container">
        <video id="video" autoplay playsinline></video>
        <canvas id="canvas"></canvas>
        <div id="status">Loading MediaPipe...</div>
        <div id="error">
            <h3>Error</h3>
            <p id="error-message"></p>
        </div>
    </div>

    <script type="module">
        import { FaceDetector, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";

        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const status = document.getElementById('status');
        const errorDiv = document.getElementById('error');
        const errorMessage = document.getElementById('error-message');

        let faceDetector;
        let runningMode = "VIDEO";
        let lastVideoTime = -1;

        function showError(message) {
            errorMessage.textContent = message;
            errorDiv.style.display = 'block';
            status.style.display = 'none';
        }

        function updateStatus(message) {
            status.textContent = message;
        }

        function resizeCanvas() {
            canvas.width = window.innerWidth;
            canvas.height = window.innerHeight;
        }

        function drawResults(results) {
            // Clear canvas
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            
            if (results.detections && results.detections.length > 0) {
                updateStatus(`Faces detected: ${results.detections.length}`);
                
                results.detections.forEach((detection, index) => {
                    // Get bounding box - MediaPipe provides pixel coordinates
                    const bbox = detection.boundingBox;
                    
                    // Scale to canvas size
                    const canvasRatio = Math.min(canvas.width / video.videoWidth, canvas.height / video.videoHeight);
                    const offsetX = (canvas.width - video.videoWidth * canvasRatio) / 2;
                    const offsetY = (canvas.height - video.videoHeight * canvasRatio) / 2;
                    
                    const x = bbox.originX * canvasRatio + offsetX;
                    const y = bbox.originY * canvasRatio + offsetY;
                    const width = bbox.width * canvasRatio;
                    const height = bbox.height * canvasRatio;

                    // Draw bounding box
                    ctx.strokeStyle = '#ffb3d9';
                    ctx.lineWidth = 3;
                    ctx.strokeRect(x, y, width, height);

                    // Draw confidence percentage
                    if (detection.categories && detection.categories.length > 0) {
                        const confidence = Math.round(detection.categories[0].score * 100);
                        
                        ctx.fillStyle = '#ffb3d9';
                        ctx.font = '18px Arial';
                        ctx.strokeStyle = '#ff0000';
                        ctx.lineWidth = 3;
                        
                        const text = `Face ${index + 1}: ${confidence}%`;
                        
                        // Save the current transformation
                        ctx.save();
                        
                        // Flip the text horizontally to reverse it
                        ctx.scale(-1, 1);
                        
                        // Draw text with outline for better visibility (adjust x position for flip)
                        ctx.strokeText(text, -(x + ctx.measureText(text).width), y - 10);
                        ctx.fillText(text, -(x + ctx.measureText(text).width), y - 10);
                        
                        // Restore the transformation
                        ctx.restore();
                    }
                });
            } else {
                updateStatus('No faces detected');
            }
        }

        async function initMediaPipe() {
            try {
                updateStatus('Loading MediaPipe model...');
                
                // Initialize the FilesetResolver
                const vision = await FilesetResolver.forVisionTasks(
                    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
                );

                // Create FaceDetector
                faceDetector = await FaceDetector.createFromOptions(vision, {
                    baseOptions: {
                        modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite',
                        delegate: "GPU"
                    },
                    runningMode: runningMode,
                    minDetectionConfidence: 0.5
                });
                
                updateStatus('MediaPipe model loaded. Starting camera...');
                await initCamera();
                
            } catch (error) {
                console.error('MediaPipe initialization error:', error);
                showError('Failed to initialize MediaPipe: ' + error.message);
            }
        }

        async function initCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        width: { ideal: 1280 },
                        height: { ideal: 720 },
                        facingMode: 'user'
                    }
                });

                video.srcObject = stream;
                
                video.onloadedmetadata = () => {
                    video.play();
                    updateStatus('Camera ready. Detecting faces...');
                    startDetection();
                };

            } catch (error) {
                console.error('Camera access error:', error);
                showError('Failed to access camera. Please ensure camera permissions are granted.');
            }
        }
        
        function startDetection() {
            const detect = async () => {
                if (video.readyState === 4 && video.videoWidth > 0) {
                    const currentTime = video.currentTime;
                    
                    // Only process if we have a new frame
                    if (currentTime !== lastVideoTime) {
                        lastVideoTime = currentTime;
                        
                        try {
                            // Detect faces in the current video frame
                            const results = faceDetector.detectForVideo(video, performance.now());
                            drawResults(results);
                        } catch (error) {
                            console.error('Detection error:', error);
                        }
                    }
                }
                requestAnimationFrame(detect);
            };
            detect();
        }

        // Handle window resize
        window.addEventListener('resize', resizeCanvas);
        
        // Initialize everything
        window.addEventListener('load', () => {
            resizeCanvas();
            initMediaPipe();
        });

        // Handle page visibility change
        document.addEventListener('visibilitychange', () => {
            if (document.hidden) {
                updateStatus('Paused (tab not visible)');
            } else if (faceDetector) {
                updateStatus('Detecting faces...');
            }
        });
    </script>
</body>
</html>